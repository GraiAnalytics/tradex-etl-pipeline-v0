```markdown
# TradeX â€“ Data Pipeline ETL

Pipeline ETL profesional y escalable para **TradeX**, orientado a la ingestiÃ³n, normalizaciÃ³n y generaciÃ³n de **seÃ±ales accionables para el mercado financiero** a partir de datos alternativos (X/Twitter, Reddit, YouTube, News/RS S, SEC, arXiv, journals) y datos de mercado (**Alpaca**).

El pipeline estÃ¡ diseÃ±ado bajo una arquitectura **Bronze / Silver / Gold**, con foco en:

- reproducibilidad
- auditorÃ­a
- escalabilidad
- observabilidad
- data quality
- backfills seguros
- idempotencia (re-ejecuciones sin duplicar)

---

## ğŸ“ Arquitectura de alto nivel

```

FUENTES EXTERNAS
(X Basic API v2, Reddit, YouTube, RSS News/Journals, SEC, arXiv, Alpaca)
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      BRONZE        â”‚  Raw, auditable, immutable (run_id + manifest + _SUCCESS)
â”‚     (JSONL.gz)     â”‚  Particionado por ingest_date, commit atÃ³mico por run
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      SILVER        â”‚  Clean, normalized, canonical (entity-resolved)
â”‚ (Parquet/Iceberg)  â”‚  Dedup semÃ¡ntico + late data + schema drift handling
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       GOLD         â”‚  Data products: metrics, baselines, features, signals
â”‚ (serving-ready)    â”‚  SeÃ±ales explicables (drivers + confidence)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
UI / API / ALERTS / ML MODELS

```

---

## ğŸ¯ Objetivos del pipeline

- Ingestar mÃºltiples fuentes heterogÃ©neas sin perder informaciÃ³n (**raw payload preservado en Bronze**)
- Normalizar y resolver entidades de forma consistente (`entity_id`)
- Calcular mÃ©tricas relativas (baselines, deltas, z-scores)
- Detectar seÃ±ales explicables (spikes, anomalÃ­as, divergencias)
- Servir datos listos para UI, alertas y modelos ML (tablas Gold â€œserving-readyâ€)
- Permitir reprocesamiento y auditorÃ­a completa (**run_id + manifest + state store**)
- Soportar incremental + backfills con la misma lÃ³gica (sin hacks)

---

## ğŸ§± Capas de datos

### ğŸŸ¤ Bronze (Raw / Ingest)

Bronze es un **log de ingestas** con commits atÃ³micos por `run_id`.

- Copia fiel de las fuentes externas (con envelope estÃ¡ndar)
- Inmutable, versionada por `run_id`
- Incluye metadata tÃ©cnica (`ingest_time`, `event_time`, `cursor`, `params`)
- Manifiesto por corrida (`manifest.json`) y marcador `_SUCCESS`
- Idempotencia: dedup in-run + dedup persistente (Postgres) para evitar duplicados en re-runs/backfills

Ejemplos (dataset_key â†’ path):
- `x.posts` â†’ `bronze/source_system=x/dataset=posts/...`
- `market.daily_prices` â†’ `bronze/source_system=market/dataset=daily_prices/...`
- `sec.filings` â†’ `bronze/source_system=sec/dataset=filings/...`

**EspecificaciÃ³n exacta**: `docs/architecture/01_bronze.md`

---

### âšª Silver (Clean / Canonical)

Silver define el **modelo canÃ³nico**: tipado fuerte + normalizaciÃ³n + consistencia inter-fuentes.

- Datos limpios, tipados y normalizados
- ResoluciÃ³n de entidades (`entity_id`) y mapeos consistentes
- DeduplicaciÃ³n semÃ¡ntica (ademÃ¡s del dedup por ID)
- Manejo de late data (reprocessing window) y schema drift controlado
- Base confiable para anÃ¡lisis, features y entrenamiento ML

Tablas core:
- `silver_entity_master`
- `silver_social_events`
- `silver_market_daily`

---

### ğŸŸ¡ Gold (Business / Signals)

Gold son **data products** listos para consumo.

- MÃ©tricas finales (market + social)
- Baselines y features versionadas (`feature_version`, `signal_version`)
- SeÃ±ales detectadas con drivers, severidad y confianza
- Serving tables optimizadas para UI, feeds y alertas

Ejemplos:
- `gold_market_entity_day`
- `gold_social_entity_day`
- `gold_baselines_entity_day`
- `gold_signal_events`
- `gold_entity_daily_summary`

---

## ğŸ§© Datasets soportados (v0)

### Social / Alt-data
- **X (Twitter) â€“ Basic (API v2)**:
  - `x.posts` (recent search con cursor `start_time` + lookback + dedup)
  - opcional: `x.counts`, `x.usage` (si los habilitas)
- `reddit.posts`
- `youtube.comments`
- `news.articles_rss`
- `journals.articles_rss`
- `sec.filings`
- `arxiv.papers`

### Market
- **Alpaca**:
  - `market.daily_prices`

---

## ğŸ› ï¸ Stack tecnolÃ³gico (default)

- **Lenguaje**: Python 3.11+
- **OrquestaciÃ³n**: Dagster (Python-first, asset/job based)
- **Storage**: S3-compatible (MinIO dev / S3 prod)
- **Formato**:
  - Bronze: **JSONL.gz** (run-based, auditable)
  - Silver/Gold: Parquet (+ Iceberg/Delta opcional segÃºn engine)
- **Estado/Metadata**: Postgres (prod y local via docker)
- **Data Quality**: contracts + quality gates + quarantine
- **Observabilidad**: logs estructurados + mÃ©tricas por run (counts, latency, errors, drift)

---

## ğŸ“ Estructura del repositorio (resumen)

```

src/tradex_pipeline/
â”œâ”€ sources/        # extractores por fuente (X, Alpaca, SEC, RSS, etc.)
â”œâ”€ bronze/         # runner genÃ©rico + envelopes + manifests + writer + dedup
â”œâ”€ silver/         # parsers + transforms + quality + writer
â”œâ”€ gold/           # metrics + baselines + features + signals + serving
â”œâ”€ contracts/      # definiciÃ³n de datasets, llaves, particiones, versiones
â”œâ”€ state/          # pipeline_state + bronze_runs + bronze_record_index (Postgres)
â”œâ”€ orchestration/  # dagster resources/jobs/schedules
â”œâ”€ alerts/         # dispatch de alertas (webhook/kafka/slack opcional)
â””â”€ common/         # utils compartidos

````

La estructura completa y las decisiones estÃ¡n documentadas en `/docs/architecture`.

---

## ğŸš€ Quickstart (local)

### 1ï¸âƒ£ Requisitos

- Python 3.11+
- Docker + Docker Compose
- Poetry
- Make (opcional, recomendado)

---

### 2ï¸âƒ£ Clonar el repo

```bash
git clone git@github.com:tradex-ai/tradex-pipeline-etl.git
cd tradex-pipeline-etl
````

---

### 3ï¸âƒ£ Variables de entorno

```bash
cp .env.example .env
```

Completa como mÃ­nimo:

* credenciales de X (Bearer Token) â€” **Basic API v2**
* credenciales de Alpaca (API Key / Secret + endpoint market data)
* credenciales de storage (MinIO/S3)
* configuraciÃ³n Postgres

---

### 4ï¸âƒ£ Levantar stack local

```bash
docker-compose up -d
```

Esto levanta:

* MinIO (object storage S3-compatible)
* Postgres (state store + metadata)
* servicios auxiliares (segÃºn compose)

---

### 5ï¸âƒ£ Instalar dependencias

```bash
poetry install
poetry shell
```

---

## â–¶ï¸ EjecuciÃ³n de pipelines

> Nota: usamos `dataset_key` del estilo `source.dataset` (ej: `x.posts`, `market.daily_prices`).

### Ingesta Bronze

```bash
python -m tradex_pipeline.cli run bronze x.posts
python -m tradex_pipeline.cli run bronze market.daily_prices
```

### ConstrucciÃ³n Silver

```bash
python -m tradex_pipeline.cli run silver social_events
python -m tradex_pipeline.cli run silver market_daily
```

### ConstrucciÃ³n Gold

```bash
python -m tradex_pipeline.cli run gold build_metrics
python -m tradex_pipeline.cli run gold detect_signals
python -m tradex_pipeline.cli run gold build_serving
```

---

## ğŸ” Backfills

Ejemplo: backfill de X posts por rango de fechas

```bash
python -m tradex_pipeline.cli backfill \
  --layer bronze \
  --dataset x.posts \
  --start-date 2025-12-01 \
  --end-date 2025-12-15
```

Reglas:

* Bronze crea nuevos `run_id` (no pisa runs previos)
* Dedup persistente evita duplicados
* Silver/Gold se regeneran desde Bronze sin tocar fuentes externas

---

## ğŸ§ª Data Quality

* Cada dataset tiene **contracts** en `src/tradex_pipeline/contracts/`
* Silver y Gold aplican **quality gates**
* Registros invÃ¡lidos van a **quarantine**
* Fallos crÃ­ticos bloquean publicaciÃ³n aguas abajo (no â€œdatos a mediasâ€)

---

## ğŸ“Š Observabilidad

Por cada `run_id` se registran:

* volumen de datos (fetched / written / deduped)
* latencia por etapa (fetch / write / commit)
* errores por tipo (429, 5xx, timeout)
* lag de event_time (si aplica)
* drift de esquema (fingerprints)

Alertas automÃ¡ticas ante:

* fallos consecutivos
* caÃ­das bruscas de volumen
* schema drift significativo

---

## ğŸ” Seguridad

* Secrets vÃ­a `.env` / Secret Manager
* Storage cifrado en reposo
* Acceso por roles a Bronze/Silver/Gold
* RetenciÃ³n configurable por capa (polÃ­ticas por entorno)

---

## ğŸ§  FilosofÃ­a de diseÃ±o

* **Bronze nunca se modifica**
* **Silver es la verdad canÃ³nica**
* **Gold es un data product**
* Todo es:

  * versionado
  * reproducible
  * explicable
  * auditable
* Agregar una nueva fuente debe ser mayormente:

  * extractor + config + contract (no â€œun script nuevoâ€)